{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import operator\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from os import listdir, environ\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "from tensorflow.python.keras.models import Input, Model, load_model\n",
    "from tensorflow.python.keras.layers import LSTM, Dropout, Dense,Embedding,SpatialDropout1D,GRU\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_story(object):\n",
    "    def __init__(self,interval,path,epoch,keep_training=False):\n",
    "        self.interval = interval\n",
    "        self.path = path\n",
    "        self.story_raw, self.sentence_raw, self.text_raw = self.load()\n",
    "        self.x,self.y, self.x_sentence,self.y_sentence = self.convert(self.story_raw, self.sentence_raw)\n",
    "        \n",
    "        self.d, self.wordlist, self.uncommon,self.tokenizer = self.token(self.story_raw)\n",
    "        \n",
    "        self.reverse_d, self.embedding, self.uncommon_words = self.embed(self.tokenizer)\n",
    "        self.X, self.Y = self.final_prepare(self.story_raw,self.tokenizer)\n",
    "        self.model, self.callbacks_list = self.build_model()\n",
    "        self.epoch = epoch \n",
    "        \n",
    "        self.path = \"model\" + str(self.interval) + \".h5\"\n",
    "        if os.path.exists(self.path):\n",
    "            self.model,self.callbacks_list = self.build_model()\n",
    "            print(self.model)\n",
    "            self.model.load_weights(self.path)\n",
    "        else:\n",
    "            self.model,self.callbacks_list = self.build_model()\n",
    "            self.train()\n",
    "        \n",
    "\n",
    "        if keep_training:\n",
    "            self.train()\n",
    "        \n",
    "        \n",
    "    def clean(self,text):\n",
    "    \n",
    "        out = copy.deepcopy(text)\n",
    "        out = out.lower()\n",
    "        out = out.replace('\\d',\" \")\n",
    "        out = out.replace(\"\\t\", \" \")\n",
    "        out = out.replace(\"\\n\", \" \")\n",
    "        for char in \"!#()-./:-_\"\"\":\n",
    "            out = out.replace(char,\" \")\n",
    "        out = out.replace(\"'\", \"'\")\n",
    "        out = out.replace(',', \" , \")\n",
    "        out = out.replace('.', \" . \")\n",
    "        out = out.replace(';', \" ; \")\n",
    "        out = out.replace('?', \" ? \")\n",
    "        out = out.replace('‘', \"'\")\n",
    "        out = out.replace('…', \" . \")\n",
    "        out = out.replace('ç', \"c\")\n",
    "        out = out.replace('é', \"e\")\n",
    "        out = out.replace('\"', \" \")\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    def load(self,p = \"Stories/\",words_min=10):\n",
    "        print(\"loading stories.....\")\n",
    "        print(p)\n",
    "        files = [i for i in listdir(p) if \"txt\" in i]\n",
    "        story_raw = [];\n",
    "        sentence_raw = [];\n",
    "        text_raw = \"\";\n",
    "        for file in tqdm_notebook(files):\n",
    "            loc = p + file\n",
    "            story = open(loc).read()\n",
    "            story = self.clean(story)\n",
    "            sentences = re.split(\"\\.|\\?\", story)\n",
    "            for sentence in sentences:\n",
    "                l = sentence.strip().split()\n",
    "                if len(l) > words_min:\n",
    "                    sentence_raw.append(sentence)\n",
    "            story_raw.append(story)\n",
    "            text_raw += story\n",
    "        return story_raw, sentence_raw, text_raw\n",
    "    \n",
    "    def convert(self,story_raw, sentence_raw):\n",
    "        x = []\n",
    "        y = []\n",
    "        x_sentence = []\n",
    "        y_sentence = []\n",
    "        for story in story_raw:\n",
    "            words = story.split()\n",
    "            for i in range(0,len(words) - self.interval,self.interval):\n",
    "                if i + self.interval >= len(words):\n",
    "                    final = len(words) - 1\n",
    "                else:\n",
    "                    final = i + self.interval\n",
    "                x.append(\" \".join(words[i:final]))\n",
    "                y.append(words[final])\n",
    "        final = 0 \n",
    "        for sentence in sentence_raw:\n",
    "            words = sentence.split()\n",
    "            words.append(\".\")\n",
    "            for i in range(len(words) - self.interval):\n",
    "                if i + self.interval >= len(words):\n",
    "                    final = len(words) - 1\n",
    "                else:\n",
    "                    final = i + self.interval\n",
    "                x_sentence.append(\" \".join(words[i:final]));\n",
    "                y_sentence.append(words[final])\n",
    "        return x,y, x_sentence,y_sentence\n",
    "    \n",
    "    def token(self,story_raw,min_appear=2):\n",
    "        tokenizer = Tokenizer(filters=\"\")\n",
    "        tokenizer.fit_on_texts(story_raw)\n",
    "    \n",
    "        d = tokenizer.word_counts\n",
    "        d = sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        wordlist, uncommon = [], []\n",
    "        for k,v in d:\n",
    "            if v > min_appear:\n",
    "                wordlist.append(k);\n",
    "            else:\n",
    "                uncommon.append(k);\n",
    "        return d, wordlist, uncommon,tokenizer\n",
    "    \n",
    "    def embed(self,tokenizer):\n",
    "        glovePath = \"GloveData/\"\n",
    "        embedding_d = {}\n",
    "        f = open(\"GloveData/tingle-vectors-300.txt\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            w = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_d[w] = vector\n",
    "        f.close()\n",
    "\n",
    "        embedding = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "        uncommon_words = []\n",
    "        reverse = {}\n",
    "        for word, ind in tokenizer.word_index.items():\n",
    "            reverse[ind] = word\n",
    "            if word in embedding_d:\n",
    "                embedding[ind] = embedding_d[word]\n",
    "            else:\n",
    "                uncommon_words.append(word)\n",
    "    \n",
    "        return reverse, embedding, uncommon_words\n",
    "    \n",
    "    def final_prepare(self,story_raw,tokenizer):\n",
    "        all_words= []\n",
    "        for story in story_raw:\n",
    "            s = story.split()\n",
    "            all_words.append(s)\n",
    "        all_words_flat = [item for sublist in all_words for item in sublist]\n",
    "    \n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(len(all_words_flat)-self.interval):\n",
    "            X.append(all_words_flat[i:i+self.interval])\n",
    "            Y.append(all_words_flat[i+self.interval])\n",
    "    \n",
    "        X = tokenizer.texts_to_sequences(X)\n",
    "        Y = tokenizer.texts_to_sequences(Y)\n",
    "        Y = np_utils.to_categorical(Y, num_classes=len(tokenizer.word_index) + 1)\n",
    "    \n",
    "        return X, Y \n",
    "    \n",
    "    def build_model(self):\n",
    "        input_ = Input(shape=(self.interval,))\n",
    "\n",
    "        emb = Embedding(len(self.tokenizer.word_index)+1, 300, weights=[self.embedding], trainable=True)(input_)\n",
    "\n",
    "        lstm_2 = GRU(256)(emb)\n",
    "        lstm_2 = Dropout(0.2)(lstm_2)\n",
    "\n",
    "        out = Dense(len(self.tokenizer.word_index)+1, activation='softmax')(lstm_2)\n",
    "        model = Model(input_, out)\n",
    "        opt = Adam(lr=0.002)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        filepath=\"model\" + str(self.interval) + \".h5\"\n",
    "        checkpoint = ModelCheckpoint(filepath, save_weights_only=True),\n",
    "        callbacks_list = [checkpoint]\n",
    "\n",
    "        \n",
    "        return model, callbacks_list\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.fit(self.X,self.Y, batch_size=128,epochs=self.epoch, \n",
    "                    callbacks=self.callbacks_list)\n",
    "    \n",
    "    def predict(self,sen):\n",
    "        x = sen\n",
    "        x2 = self.clean(x)\n",
    "        x2 = x2.split()\n",
    "        x2 = self.tokenizer.texts_to_sequences([x2])\n",
    "        x_final = x2[0]\n",
    "        x_final = x_final[len(x_final)-7:]\n",
    "        x_final = np.array(x_final)\n",
    "        out = \"\"\n",
    "        end = 100\n",
    "        cur = x_final\n",
    "        i=0\n",
    "\n",
    "        while i < end:\n",
    "            cur = cur[1:]\n",
    "            pred = self.model.predict(cur.reshape((1,6)))\n",
    "            pred_ind = np.argmax(pred)\n",
    "            cur = np.append(cur,pred_ind)\n",
    "            out += self.reverse_d[pred_ind] + \" \"\n",
    "            i += 1\n",
    "        \n",
    "        return sen + out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading stories.....\n",
      "Stories/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc86cf805add4ce0980560cb8a4a9133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=134), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<tensorflow.python.keras.engine.training.Model object at 0x65f83bda0>\n"
     ]
    }
   ],
   "source": [
    "generator = generate_story(6,\"Stories/\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"my heart pounding within my chest and now he's just about ready to blow his load as i lay my eyes open and start to focus on the natural beauty , and this is reality this kind of photos that we will make their way into the main room , and my heart is flooded with a mixture of pain and pleasure , my entire body is still going to be the case i was met a unicorn i tell him , my legs shaking and convulsing wildly , ready to controlling myself as i scream to throw the bottom of my back and then continue down \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.predict(\"my heart pounding within my chest and \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
